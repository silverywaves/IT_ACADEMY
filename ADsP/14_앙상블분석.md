## 앙상블 분석(Ensemble)
### 1. 앙상블
#### 정의, 특징
- 여러개의 분류모형에 의한 결과 종합 -> 분류 정확도 높임
- 적절한 표본추출법으로 데이터에서 여러개의 훈련용 데이터 집합을 생성 -> 각 데이터 집합에 하나의 분류기를 만들어 결합
- 성능 분산 -> 과적합 감소


#### 종류
1. Voting 
  - 서로 다른 여러개 알고리즘 분류기 사용

![voting](https://github.com/silverywaves/IT_ACADEMY/assets/155939946/9a66f671-b2e4-4076-8136-04487dc258e7)


2. Bagging(배깅)
  - 서로 다른 훈련 데이터 샘플로 훈련, 서로 같은 알고리즘 분류기 결합
  - 원 데이터에서 중복을 허용하는 크기가 같은 표본을 여러 단순 임의 복원 추출하여 각 표본에 대해 분류기 생성
  - 여러 모델이 **병렬**로 학습, 그 결과를 집계하는 방식
  - 같은 데이터가 여러번 추출될 수도 있고, 어떤 데이터는 추출되지 않을 수도(Bootstrap) 있음
  - 대표적 알고리즘 : Meta Cost Algorithm


![bagging](https://github.com/silverywaves/IT_ACADEMY/assets/155939946/79b05711-c277-435e-8d30-3dc313e3565e)


3. Boosting(부스팅)
  - 여러모델이 **순차적**으로 학습
  - 이전 모델의 결과에 따라 다음 모델 표본 추출에서 분류가 잘못된 데이터에 가중치(weight)를 부여하여 표본 추출
  - 맞추기 어려운 문제를 맞추는데 초점, 이상치(outlier)에 약함
  - 대표적 알고리즘 : AdaBoost, GradientBoost, XGBoost, LightGBM(Leaf-wise-node 방법 사용)

![boosting](https://github.com/silverywaves/IT_ACADEMY/assets/155939946/fe4065a2-250f-4548-a5e7-feafeae3bb23)



4. Random Forest(랜덤 포레스트)
  - 배깅 + 랜덤 과정 추가
  - Node 내 데이터를 자식노드로 나누는 기준을 정할때 모든 예측변수에서 분할을 선택하는대신, 설명변수의 일부분만을 고려함으로 성능 높임
  - 여러개의 의사결정 나무를 사용 -> 하나의 나무를 사용할때보다 과적합 문제 피할 수 있음


![랜덤포레스트](https://github.com/silverywaves/IT_ACADEMY/assets/155939946/aed6214e-5600-4ab6-9f6c-18d34a9a8f85)


5. K-NN(K-Nearest Neighbors)
  - 새로운 데이터에 대해 주어진 이웃의 개수(k)만큼 가까운 멤버들과 비교하여 결과를 판단
  - k 값에 따라 소속되는 그룹이 달라질 수 있음 (k 값은 hyper parameter)
  - 거리를 측정해 이웃을 뽑기때문에 스케일링이 중요
  - 반응변수가 범주형이면 분류, 연속형이면 회귀 목적으로 사용
  - 모형을 미리 만들지 않고 새로운 데이터가 들어오면 그때부터 계산 시작(Lazy Learning-게으른 학습) => 지도학습 알고리즘

![knn](https://github.com/silverywaves/IT_ACADEMY/assets/155939946/2537e8ac-251a-4d21-a18f-abeee232ad29)


6. SVM(Support Vector Machine)
   - 서로 다른 분류에 속한 데이터 간격이 최대가 되는 선을 찾아 이를 기준으로 데이터를 분류
   - 아래 그림에서 H3은 분류를 올바르게 못하였음
   - H1, H2는 올바름. 간격(margin)이 H2가 더 크므로 H2가 분류기준

![svm](https://github.com/silverywaves/IT_ACADEMY/assets/155939946/25ec6974-7be6-4f20-83b2-3d1f18d6901a)


