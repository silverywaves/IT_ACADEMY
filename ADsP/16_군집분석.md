## 군집분석(Clustering)
### 1. 군집분석의 종류
#### 군집분석

- 여러 변수 값들로부터 n개의 개체를 유사한 성격을 가지는 몇 개의 군집으로 집단화
- 형성된 군집들의 특성을 파악해 군집간 관계 분석


---

### 2. 계층적 군집(Hierachical Clustering)

- 가장 유사한 개체를 묶는 과정을 반복하여 원하는 개수의 군집 형성
- 유사도 판단은 두 개체간 거리에 기반하므로 거리측정에 대한 정의 필요
  - 유클리드, 맨해튼, 민코프스키, 마할라노비스
- 이상치에 민감(거리기반이기 때문)
- 사전에 k군집수를 설정할 필요 X. 탐색적 모형
- 병합적 방법에서 한번 군집이 형성되면, 그 안의 개체는 이동 불가
- hclust() 함수, cluster 패키지의 agnes(), mclust() 함수 사용


```
  💌 계층적 군집 - 응집형 군집 방법
    1. 최단연결법 : 단일연결법, 거리의 최소값, 고립된 군집을 찾는데 중점
    2. 최장연결법 : 완전연결법, 거리의 최대값
    3. 중심연결법 : 중심간의 거리 측정, 군집결합시 평균은 가중평균으로 구함
    4. 와드연결법 : 오차제곱합에 기초하여 군집 수행. 크기 비슷한 군집끼리 병합 경향
    5. 평균연결법 : 모든 항목에 거리 평균 구하면서 군집화 -> 계산량 많아짐
```


```
  💠 계측정 군집의 거리
    - 수학적 거리의 개념 : 유클리드, 맨해튼, 민코프스키
      1. 유클리드 : 가장 일반적, 방향 고려 X
      2. 맨해튼 : 두 점의 각 성분 차의 절대값 합
      3. 민코프스키 : 거리차수와 함께 사용, 일반적으로 사용하는 차수 1, 2, 무한대
                   : q = 2 이면 유클리드, q = 1 이면 맨해튼 거리

    - 통계적 거리의 개념 : 표준화, 마할라노비스
      1. 표준화거리 : 각 변수를 표준편차로 척도 변환한 후 유클리드 거리 계산
      2. 마할라노비스 : 변수의 표준화와 함께 변수간 상관성을 동시에 고려한 통계적 거리
```


---

### 3. 비계층적 군집 - 분할적 군집방법

#### K-중심군집

- k-means 장단점
  - k-mean 방법은 사전에 군집수 k를 정해줌 (k : hyperparameter)
  - 군집수 k가 원데이터 구조에 적합해야함 (if not -> 결과 bad)
  - 알고리즘이 단순, 빠름. 계층적 군집보다 많은 양의 자료 처리
  - k-means 군집은 잡음, 이상값에 영향 받기 쉬움.
    - 분석 전 이상값 제거하면 좋음
  - 평균 대신 중앙값을 사용하는 k-medoids 군집 사용 가능


- k-means 정차
  - 초기 군집의 중심으로 k개의 객체를 임의 선택
  - 각 자료의 가장 가까운 군집의 중심에 할당
  - 각 군집 내 자료들의 평균 계산 -> 군집 중심 갱신
  - 군집 중심의 변화가 없을때까지 2~3회 반복


#### DBSCAN

  - 밀도기반, 밀도가 높은 부분을 클러스터링
  - 반경 내 점이 n개 이상 있으면 하나의 군집으로 인식
  - Gaussian 분포가 아닌 임의적 모양의 군집분석에 적합
  - k값 정할 필요 X, outlier에 의한 성능 하락 완화


---

### 4. 혼합분포군집

데이터 peak가 2개 등 복잡한 구조. 모수 & 가중치 추정 EM 알고리즘

- EM 알고리즘 진행과정
  - 모수(평균, 분산, 혼합계수)에 대해 임의의 초기값 정함
  - E STEP : k개의 모형 군집에 대해 모수를 사용해 각 군집에 속할 사후확률 구함
  - M STEP : 사후확률을 이용해 최대 우도 추정으로 모수 재추정, 과정 반복
 

---

### 5. 군집화 평가지수

  - 실루엣 계수(Silhouette Coefficient)
    - 군집 내 거리와 군집간 거리를 기준으로 군집 분할 성과 측정
    - 클러스터 안의 데이터들이 다른 클러스터와 비교해 얼마나 비슷한가를 나타냄
    - 실루엣 지표가 1에 가까울수록 잘 군집화되었음
    - 실루엣 지표가 0.5 이상이면 결과가 타당한 것으로 평가

  - Dunn Index(DI)
    - DI : 군집과 군집 사이 거리 中 MIN / 군집 내 데이터 거리 中 MAX
    - 분자가 클수록 군집간 거리 멀고, 분모가 작을수록 군집내 data 몰림
    - DI 클수록 잘 군집화 됨
   

---

### 6. SOM(Self-Organization Maps)

  - 자기조직화지도
  - 인공신경망의 한 종류, 차원축소와 군집화를 동시에 수행
  - 비지도 학습(Unsupervised Learning)
  - 고차원으로 표현된 데이터를 저차원으로 변환
  - 입력층과 2차원 격자 형태의 경쟁층으로 이루어짐 (2개의 층)

<br>

1. SOM PROCESS
    - SOM 의 노드에 대한 연결강도(weight) 초기화
    - 입력 벡터와 경쟁층 노드간의 거리계산 및 *입력벡터와 가까운 노드* 선택 -> 경쟁
                                    *선택된 프로토타입벡터 : BMU(Best-Matching Unit)*
    - 경쟁에서 선택된 노드와 이웃노드의 가중치(연결강도) 갱신 -> 협력 및 적응
    - 두번째 단계로 가서 반복
   ✔ 경쟁학습 : 승자만이 출력을 내고, 승자와 그의 이웃만이 연결강도를 수정하는 승자 독점 구조로 인해 경쟁층에는 승자 뉴런만 나타냄

2. SOM **vs** 신경망모형
   - 신경망 모형은 연속적인 layer로 구성 **vs** SOM은 2차원 격자 구성
   - 신경망 모형은 에러 수정을 학습 **vs** SOM은 경쟁 학습 실시
   - 신경망 모형은 역전파 알고리즘 **vs** SOM은 전방패스를 사용해 속도가 매우 빠름
  
   
